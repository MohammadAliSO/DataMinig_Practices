{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1 : Map Reduce (movie_id, unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "ratings_df = pd.read_csv('./data/ratings.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify loading\n",
    "print(ratings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId  userId\n",
      "0        1       1\n",
      "1        3       1\n",
      "2        6       1\n",
      "3       47       1\n",
      "4       50       1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Map Phase\n",
    "# For Map phase, we extract relevant columns: 'movieId' and 'userId'\n",
    "map_df = ratings_df[['movieId', 'userId']]\n",
    "\n",
    "# Display the first few rows of the map_df to verify extraction\n",
    "print(map_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId  unique_users\n",
      "0        1           215\n",
      "1        2           110\n",
      "2        3            52\n",
      "3        4             7\n",
      "4        5            49\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Reduce Phase\n",
    "# Group by 'movieId' and count unique 'userId'\n",
    "reduce_df = map_df.groupby('movieId')['userId'].nunique().reset_index()\n",
    "\n",
    "# Rename columns to match the desired output\n",
    "reduce_df.columns = ['movieId', 'unique_users']\n",
    "\n",
    "# Display the first few rows of the reduce_df to verify results\n",
    "print(reduce_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId  unique_users\n",
      "0           1           215\n",
      "1           2           110\n",
      "2           3            52\n",
      "3           4             7\n",
      "4           5            49\n",
      "...       ...           ...\n",
      "9719   193581             1\n",
      "9720   193583             1\n",
      "9721   193585             1\n",
      "9722   193587             1\n",
      "9723   193609             1\n",
      "\n",
      "[9724 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Output the Results\n",
    "# Save the results to a CSV file\n",
    "reduce_df.to_csv('./result/task_1_unique_users_per_movie.csv', index=False)\n",
    "\n",
    "# Optionally, display the entire result\n",
    "print(reduce_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2 : MapReduce  top-rated movies (movie_id, average_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "ratings_df = pd.read_csv('./data/ratings.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify loading\n",
    "print(ratings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId  rating\n",
      "0        1     4.0\n",
      "1        3     4.0\n",
      "2        6     4.0\n",
      "3       47     5.0\n",
      "4       50     5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Map Phase\n",
    "# For Map phase, we extract relevant columns: 'movieId' and 'rating'\n",
    "map_df = ratings_df[['movieId', 'rating']]\n",
    "\n",
    "# Display the first few rows of the map_df to verify extraction\n",
    "print(map_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Reduce Phase\n",
    "# Group by 'movieId' and calculate the average rating\n",
    "reduce_df = map_df.groupby('movieId')['rating'].mean().reset_index()\n",
    "\n",
    "# Rename columns to match the desired output\n",
    "reduce_df.columns = ['movieId', 'average_rating']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId  average_rating\n",
      "7638    88448             5.0\n",
      "8089   100556             5.0\n",
      "9065   143031             5.0\n",
      "9076   143511             5.0\n",
      "9078   143559             5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Sort and Output the Results\n",
    "# Sort by 'average_rating' in descending order\n",
    "sorted_df = reduce_df.sort_values(by='average_rating', ascending=False)\n",
    "\n",
    "# Display the first few rows of the sorted_df to verify results\n",
    "print(sorted_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movieId  average_rating\n",
      "7638    88448             5.0\n",
      "8089   100556             5.0\n",
      "9065   143031             5.0\n",
      "9076   143511             5.0\n",
      "9078   143559             5.0\n",
      "...       ...             ...\n",
      "9253   157172             0.5\n",
      "7536    85334             0.5\n",
      "6486    53453             0.5\n",
      "5200     8494             0.5\n",
      "7145    71810             0.5\n",
      "\n",
      "[9724 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the results to a CSV file\n",
    "sorted_df.to_csv('./result/task_2_top_rated_movies.csv', index=False)\n",
    "\n",
    "# Optionally, display the entire result\n",
    "print(sorted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: similar Movie MinHash Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasketch in c:\\users\\mohammadali\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.4)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\mohammadali\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasketch) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\mohammadali\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasketch) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the Dataset\n",
    "ratings_df = pd.read_csv('./data/ratings.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify loading\n",
    "print(ratings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Create User-Movie Sets\n",
    "# Create a dictionary where key is movieId and value is the set of userIds who rated the movie\n",
    "movie_user_dict = ratings_df.groupby('movieId')['userId'].apply(set).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Generate MinHash Signatures\n",
    "# Define the number of permutations\n",
    "num_permutations = 128\n",
    "\n",
    "# Create a dictionary to store MinHash signatures for each movie\n",
    "minhash_dict = {}\n",
    "\n",
    "for movie_id, users in movie_user_dict.items():\n",
    "    m = MinHash(num_perm=num_permutations)\n",
    "    for user in users:\n",
    "        m.update(str(user).encode('utf8'))\n",
    "    minhash_dict[movie_id] = m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Create MinHash LSH\n",
    "# Define LSH with an appropriate threshold\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=num_permutations)\n",
    "\n",
    "# Add MinHash signatures to LSH\n",
    "for movie_id, m in minhash_dict.items():\n",
    "    lsh.insert(movie_id, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies similar to movie ID 1: [260, 5349, 356, 296, 3114, 1291, 1196, 780, 590, 589]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Find Similar Movies\n",
    "def find_similar_movies(movie_id, num_results=10):\n",
    "    if movie_id not in minhash_dict:\n",
    "        print(f\"Movie ID {movie_id} not found in dataset.\")\n",
    "        return []\n",
    "    \n",
    "    result = lsh.query(minhash_dict[movie_id])\n",
    "    result.remove(movie_id)  # Remove the given movie itself from the results\n",
    "    return result[:num_results]\n",
    "\n",
    "# Example: Find movies similar to a given movie ID\n",
    "given_movie_id = 1  # Replace with the movieId you are interested in\n",
    "similar_movies = find_similar_movies(given_movie_id, num_results=10)\n",
    "\n",
    "# Print similar movies\n",
    "print(f\"Movies similar to movie ID {given_movie_id}: {similar_movies}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task4 : Find closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the Dataset\n",
    "ratings_df = pd.read_csv('./data/ratings.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify loading\n",
    "print(ratings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Create User-Movie Sets\n",
    "# Create a dictionary where key is movieId and value is the set of userIds who rated the movie\n",
    "movie_user_dict = ratings_df.groupby('movieId')['userId'].apply(set).to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate MinHash Signatures\n",
    "# Define the number of permutations\n",
    "num_permutations = 128\n",
    "\n",
    "# Create a dictionary to store MinHash signatures for each movie\n",
    "minhash_dict = {}\n",
    "\n",
    "for movie_id, users in movie_user_dict.items():\n",
    "    m = MinHash(num_perm=num_permutations)\n",
    "    for user in users:\n",
    "        m.update(str(user).encode('utf8'))\n",
    "    minhash_dict[movie_id] = m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Create MinHash LSH\n",
    "# Define LSH with an appropriate threshold\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=num_permutations)\n",
    "\n",
    "# Add MinHash signatures to LSH\n",
    "for movie_id, m in minhash_dict.items():\n",
    "    lsh.insert(movie_id, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies similar to movie ID 1: [260, 5349, 356, 296, 3114, 1291, 1196, 780, 590, 589]\n",
      "Movies similar to movie ID 2: [3489, 610, 420, 6, 2054, 364, 3052, 367, 344]\n",
      "Movies similar to movie ID 3: []\n",
      "Movies similar to movie ID 4: []\n",
      "Movies similar to movie ID 5: [637, 3988, 1367]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Find Similar Movies for Multiple Movies\n",
    "def find_similar_movies(movie_ids, num_results=10):\n",
    "    similar_movies_dict = {}\n",
    "    for movie_id in movie_ids:\n",
    "        if movie_id not in minhash_dict:\n",
    "            print(f\"Movie ID {movie_id} not found in dataset.\")\n",
    "            continue\n",
    "        result = lsh.query(minhash_dict[movie_id])\n",
    "        result.remove(movie_id)  # Remove the given movie itself from the results\n",
    "        similar_movies_dict[movie_id] = result[:num_results]\n",
    "    return similar_movies_dict\n",
    "\n",
    "# Example: Find movies similar to five arbitrary movie IDs\n",
    "given_movie_ids = [1, 2, 3, 4, 5]  # Replace with the movieIds you are interested in\n",
    "similar_movies = find_similar_movies(given_movie_ids, num_results=10)\n",
    "\n",
    "# Print similar movies\n",
    "for movie_id, similar in similar_movies.items():\n",
    "    print(f\"Movies similar to movie ID {movie_id}: {similar}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
